{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npG0joOUoYUu"
      },
      "source": [
        "\n",
        "\n",
        "# Importing Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhJ7Vnj5hEW_",
        "outputId": "aa88e6c5-1099-494e-ddab-6926cc461bfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dWD-XtetWdQ",
        "outputId": "fadb9a45-1854-424b-be82-e159f3d79d0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-3.2.0 sacrebleu-2.5.1\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement RougeScorer (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for RougeScorer\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install sacrebleu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Krg5Hfl7oRN_",
        "outputId": "bd23a824-eaba-4c24-d3d7-afabd6556c45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import re\n",
        "import json\n",
        "import random\n",
        "import os\n",
        "import csv\n",
        "import math\n",
        "import time\n",
        "import sys\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Tuple\n",
        "from sacrebleu.metrics import BLEU, CHRF\n",
        "from tqdm.notebook import tqdm  # Use notebook version\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "base_dir = \"/content/drive/MyDrive/NLPA2\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OQrLkcno0-m"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYzy772XopyX"
      },
      "outputs": [],
      "source": [
        "def normalize_text(text):\n",
        "\n",
        "    #Arabic Diacritical marks\n",
        "    diacritics = [\n",
        "        '\\u0610', '\\u0611', '\\u0612', '\\u0613', '\\u0614', '\\u0615', '\\u0616',\n",
        "        '\\u0617', '\\u0618', '\\u0619', '\\u061A', '\\u064B', '\\u064C', '\\u064D',\n",
        "        '\\u064E', '\\u064F', '\\u0650', '\\u0651', '\\u0652', '\\u0653', '\\u0654',\n",
        "        '\\u0655', '\\u0656', '\\u0657', '\\u0658', '\\u0659', '\\u065A', '\\u065B',\n",
        "        '\\u065C', '\\u065D', '\\u065E', '\\u065F', '\\u0670', '\\u06D6', '\\u06D7',\n",
        "        '\\u06D8', '\\u06D9', '\\u06DA', '\\u06DB', '\\u06DC', '\\u06DF', '\\u06E0',\n",
        "        '\\u06E1', '\\u06E2', '\\u06E3', '\\u06E4', '\\u06E7', '\\u06E8', '\\u06EA',\n",
        "        '\\u06EB', '\\u06EC', '\\u06ED'\n",
        "    ]\n",
        "\n",
        "    for diacritic in diacritics:\n",
        "        text = text.replace(diacritic, '')\n",
        "\n",
        "    # Strandardize Alef and Yeh Forms\n",
        "    text = re.sub('[أإآٱ]', 'ا', text)\n",
        "    text = re.sub('[يىئ]', 'ی', text)\n",
        "\n",
        "    # Remove zero-width characters\n",
        "    text = text.replace('\\u200c', '').replace('\\u200d', '')\n",
        "\n",
        "    # Normalize punctuation\n",
        "    text = text.replace('\\u2018', \"'\").replace('\\u2019', \"'\")\n",
        "    text = text.replace('\\u201C', '\"').replace('\\u201D', '\"')\n",
        "    text = text.replace('\\u2026', '...').replace('\\u2013', '-').replace('\\u2014', '-')\n",
        "    text = text.replace('`', \"'\")\n",
        "\n",
        "    return text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XMg4iPkuafA",
        "outputId": "042350e8-d4be-47cf-d20c-598f6496b706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 20000 sentences from CSV.\n",
            "✅ Successfully processed 20000 sentences\n",
            "💾 Cleaned text saved to: /content/drive/MyDrive/NLPA2/cleaned_urdu_text.txt\n",
            "\n",
            "First 5 cleaned sentences:\n",
            "1. کبھی کبھار ہی خیالی پلاو بناتا ہوں\n",
            "2. اور پھر ممکن ہے کہ پاکستان بھی ہو\n",
            "3. یہ فیصلہ بھی گزشتہ دو سال میں\n",
            "4. ان کے بلے بازوں کے سامنے ہو گا\n",
            "5. ابی جانور میں بطخ بگلا اور دوسرا ابی پرندہ شامل ہونا\n"
          ]
        }
      ],
      "source": [
        "def process_csv_file(input_file, output_file):\n",
        "    \"\"\"\n",
        "    Process the CSV file and extract cleaned Urdu sentences using pandas\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(input_file, encoding='utf-8')\n",
        "\n",
        "        sentences = df['sentence'].dropna().astype(str).tolist()\n",
        "\n",
        "        print(f\"Loaded {len(sentences)} sentences from CSV.\")\n",
        "\n",
        "        # Clean sentences\n",
        "        cleaned_sentences = []\n",
        "        for i, sentence in enumerate(sentences, start=1):\n",
        "            try:\n",
        "                cleaned = normalize_text(sentence)\n",
        "                if cleaned:\n",
        "                    cleaned_sentences.append(cleaned)\n",
        "            except Exception as e:\n",
        "                print(f\"Error cleaning line {i}: {e}\")\n",
        "\n",
        "        # Save cleaned sentences\n",
        "        pd.Series(cleaned_sentences).to_csv(output_file, index=False, header=False, encoding='utf-8')\n",
        "\n",
        "        print(f\"✅ Successfully processed {len(cleaned_sentences)} sentences\")\n",
        "        print(f\"💾 Cleaned text saved to: {output_file}\")\n",
        "\n",
        "        # Show examples\n",
        "        print(\"\\nFirst 5 cleaned sentences:\")\n",
        "        for i, s in enumerate(cleaned_sentences[:5], 1):\n",
        "            print(f\"{i}. {s}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ Error: File '{input_file}' not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error: {e}\")\n",
        "\n",
        "\n",
        "input_file = os.path.join(base_dir, \"final_main_dataset.csv\")\n",
        "output_file = os.path.join(base_dir, \"cleaned_urdu_text.txt\")\n",
        "\n",
        "process_csv_file(input_file, output_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4bbujB2pmv2"
      },
      "source": [
        "# BPE TOKENIZER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C03yDgyQplvu"
      },
      "outputs": [],
      "source": [
        "class BPETokenizer:\n",
        "    \"\"\"Byte Pair Encoding tokenizer with Right to Left support\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int = 5000):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.vocab = []\n",
        "        self.merges = []\n",
        "        self.token_to_id = {}\n",
        "        self.id_to_token = {}\n",
        "\n",
        "    def get_stats(self, words: Dict[str, int]) -> Counter:\n",
        "        pairs = Counter()\n",
        "        for word, freq in words.items():\n",
        "            symbols = word.split()\n",
        "            for i in range(len(symbols) - 1):\n",
        "                pairs[(symbols[i], symbols[i + 1])] += freq\n",
        "        return pairs\n",
        "\n",
        "    def merge_pair(self, pair: Tuple[str, str], words: Dict[str, int]) -> Dict[str, int]:\n",
        "        new_words = {}\n",
        "        bigram = ' '.join(pair)\n",
        "        replacement = ''.join(pair)\n",
        "        for word in words:\n",
        "            new_word = word.replace(bigram, replacement)\n",
        "            new_words[new_word] = words[word]\n",
        "        return new_words\n",
        "\n",
        "    def train(self, corpus: List[str], verbose: bool = True):\n",
        "        \"\"\"Train BPE on corpus\"\"\"\n",
        "        word_freqs = Counter()\n",
        "        for text in corpus:\n",
        "            word_freqs.update(text.split())\n",
        "\n",
        "        # RTL-aware: </w> at beginning\n",
        "        words = {}\n",
        "        for word, freq in word_freqs.items():\n",
        "            words['</w> ' + ' '.join(list(word))] = freq\n",
        "\n",
        "        vocab = set()\n",
        "        for word in words.keys():\n",
        "            vocab.update(word.split())\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Initial vocab: {len(vocab)} characters\")\n",
        "\n",
        "        num_merges = self.vocab_size - len(vocab)\n",
        "        for i in range(num_merges):\n",
        "            pairs = self.get_stats(words)\n",
        "            if not pairs:\n",
        "                break\n",
        "            best_pair = max(pairs, key=pairs.get)\n",
        "            words = self.merge_pair(best_pair, words)\n",
        "            self.merges.append(best_pair)\n",
        "\n",
        "        final_vocab = set()\n",
        "        for word in words.keys():\n",
        "            final_vocab.update(word.split())\n",
        "\n",
        "        self.vocab = sorted(list(final_vocab))\n",
        "        self.token_to_id = {token: idx for idx, token in enumerate(self.vocab)}\n",
        "        self.id_to_token = {idx: token for token, idx in self.token_to_id.items()}\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Final vocab: {len(self.vocab)} tokens\")\n",
        "\n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"Tokenize text using BPE\"\"\"\n",
        "        words = text.split()\n",
        "        tokens = []\n",
        "        for word in words:\n",
        "            word_tokens = ['</w>'] + list(word)\n",
        "            for pair in self.merges:\n",
        "                i = 0\n",
        "                while i < len(word_tokens) - 1:\n",
        "                    if (word_tokens[i], word_tokens[i + 1]) == pair:\n",
        "                        word_tokens = word_tokens[:i] + [''.join(pair)] + word_tokens[i + 2:]\n",
        "                    else:\n",
        "                        i += 1\n",
        "            tokens.extend(word_tokens)\n",
        "        return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swCzexeQqW1l"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBPk5O9HqWd3"
      },
      "outputs": [],
      "source": [
        "class UrduChatbotDataset(Dataset):\n",
        "    \"\"\"Dataset for Urdu chatbot with full sequence next-token prediction\"\"\"\n",
        "\n",
        "    def __init__(self, sentences, tokenizer, max_len=50):\n",
        "        self.sentences = sentences\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # Special tokens\n",
        "        self.pad_token = '<PAD>'\n",
        "        self.start_token = '<START>'\n",
        "        self.end_token = '<END>'\n",
        "        self.unk_token = '<UNK>'\n",
        "\n",
        "        for token in [self.pad_token, self.start_token, self.end_token, self.unk_token]:\n",
        "            if token not in self.tokenizer.token_to_id:\n",
        "                idx = len(self.tokenizer.token_to_id)\n",
        "                self.tokenizer.token_to_id[token] = idx\n",
        "                self.tokenizer.id_to_token[idx] = token\n",
        "\n",
        "        self.pad_idx = self.tokenizer.token_to_id[self.pad_token]\n",
        "        self.start_idx = self.tokenizer.token_to_id[self.start_token]\n",
        "        self.end_idx = self.tokenizer.token_to_id[self.end_token]\n",
        "        self.unk_idx = self.tokenizer.token_to_id[self.unk_token]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        tokens = self.tokenizer.tokenize(sentence)\n",
        "        token_ids = [self.tokenizer.token_to_id.get(t, self.unk_idx) for t in tokens]\n",
        "\n",
        "        # Split into prefix and continuation\n",
        "        if len(token_ids) > 4:\n",
        "            split_point = len(token_ids) // 2\n",
        "            src_ids = token_ids[:split_point]\n",
        "            tgt_ids = token_ids[split_point:]\n",
        "        else:\n",
        "            src_ids = token_ids[:-1]\n",
        "            tgt_ids = token_ids[-1:]\n",
        "\n",
        "        # --- TRUNCATE before padding ---\n",
        "        src_ids = src_ids[:self.max_len]\n",
        "        tgt_ids = tgt_ids[:self.max_len - 2]  # leave room for <START> and <END>\n",
        "\n",
        "        # Encoder sequence\n",
        "        encoder_ids = src_ids + [self.pad_idx] * (self.max_len - len(src_ids))\n",
        "\n",
        "        # Decoder sequences\n",
        "        decoder_input_ids = [self.start_idx] + tgt_ids\n",
        "        decoder_target_ids = tgt_ids + [self.end_idx]\n",
        "\n",
        "        # Pad to fixed length\n",
        "        decoder_input_ids = decoder_input_ids + [self.pad_idx] * (self.max_len - len(decoder_input_ids))\n",
        "        decoder_target_ids = decoder_target_ids + [self.pad_idx] * (self.max_len - len(decoder_target_ids))\n",
        "\n",
        "        # Final safety truncation\n",
        "        decoder_input_ids = decoder_input_ids[:self.max_len]\n",
        "        decoder_target_ids = decoder_target_ids[:self.max_len]\n",
        "\n",
        "        return {\n",
        "            'encoder_input': torch.tensor(encoder_ids, dtype=torch.long),\n",
        "            'decoder_input': torch.tensor(decoder_input_ids, dtype=torch.long),\n",
        "            'decoder_target': torch.tensor(decoder_target_ids, dtype=torch.long)\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYEIVSZ1qkOt"
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmZp6wrzqvZG"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Positional encoding using sine and cosine\"\"\"\n",
        "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) #Assigns sine values to even positions in the positional encoding.\n",
        "        pe[:, 1::2] = torch.cos(position * div_term) #Assigns cosine values to odd positions in the positional encoding.\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :] #Adding Positional Value\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Cl2pZ_tqwX5"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-Head Attention mechanism\"\"\"\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_len, d_model = x.size()\n",
        "        x = x.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
        "        return x.transpose(1, 2)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        return output, attention_weights\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "        Q = self.split_heads(self.W_q(query))\n",
        "        K = self.split_heads(self.W_k(key))\n",
        "        V = self.split_heads(self.W_v(value))\n",
        "        attn_output, _ = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "        attn_output = attn_output.view(batch_size, -1, self.d_model)\n",
        "        output = self.W_o(attn_output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9JSdEfNrvcW"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Position-wise Feed-Forward Network\"\"\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.linear1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBcXGRoRr0ZV"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"Single Transformer Encoder Layer\"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output = self.self_attention(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout2(ff_output))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWT16GiJr3W9"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"Single Transformer Decoder Layer\"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.cross_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
        "        self_attn_output = self.self_attention(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout1(self_attn_output))\n",
        "        cross_attn_output = self.cross_attention(x, encoder_output, encoder_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout2(cross_attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout3(ff_output))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KT6sHZLBr4UF"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\"Complete Transformer Encoder-Decoder Model\"\"\"\n",
        "    def __init__(self, vocab_size, d_model=256, num_heads=2, d_ff=1024,\n",
        "                 num_encoder_layers=2, num_decoder_layers=2, max_len=512,\n",
        "                 dropout=0.3, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "        self.encoder_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.encoder_pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
        "        self.decoder_pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_encoder_layers)\n",
        "        ])\n",
        "\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_decoder_layers)\n",
        "        ])\n",
        "\n",
        "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
        "        self._init_parameters()\n",
        "\n",
        "    def _init_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask\n",
        "\n",
        "    def make_tgt_mask(self, tgt):\n",
        "        batch_size, tgt_len = tgt.size()\n",
        "        tgt_pad_mask = (tgt != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()\n",
        "        tgt_mask = tgt_pad_mask & tgt_sub_mask\n",
        "        return tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        tgt_mask = self.make_tgt_mask(tgt)\n",
        "\n",
        "        # Encoder\n",
        "        x = self.encoder_embedding(src) * math.sqrt(self.d_model)\n",
        "        x = self.encoder_pos_encoding(x)\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x, src_mask)\n",
        "        encoder_output = x\n",
        "\n",
        "        # Decoder\n",
        "        x = self.decoder_embedding(tgt) * math.sqrt(self.d_model)\n",
        "        x = self.decoder_pos_encoding(x)\n",
        "        for layer in self.decoder_layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "        output = self.output_projection(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcZGTjMIr7-F"
      },
      "source": [
        "# Training Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGrAQsz-sAZP"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    pbar = tqdm(train_loader, desc=\"Training\")\n",
        "\n",
        "    for batch in pbar:\n",
        "        encoder_input = batch['encoder_input'].to(device)\n",
        "        decoder_input = batch['decoder_input'].to(device)\n",
        "        decoder_target = batch['decoder_target'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(encoder_input, decoder_input)\n",
        "        output = output.reshape(-1, output.size(-1))\n",
        "        target = decoder_target.reshape(-1)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "    return epoch_loss / len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZDZCyaZsD19"
      },
      "outputs": [],
      "source": [
        "def validate(model, val_loader, criterion, device):\n",
        "    \"\"\"Validate the model\"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(val_loader, desc=\"Validation\")\n",
        "        for batch in pbar:\n",
        "            encoder_input = batch['encoder_input'].to(device)\n",
        "            decoder_input = batch['decoder_input'].to(device)\n",
        "            decoder_target = batch['decoder_target'].to(device)\n",
        "\n",
        "            output = model(encoder_input, decoder_input)\n",
        "            output = output.reshape(-1, output.size(-1))\n",
        "            target = decoder_target.reshape(-1)\n",
        "            loss = criterion(output, target)\n",
        "            epoch_loss += loss.item()\n",
        "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "    return epoch_loss / len(val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gth8M_10GjwL"
      },
      "outputs": [],
      "source": [
        "def eval_bleu_chrf(\n",
        "    model, tokenizer, dataset, sentences,\n",
        "    max_gen_len, device, sample_size=1000, use_greedy=True, set_eval=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate the 'first-half → second-half' objective:\n",
        "\n",
        "    - Tokenize with tokenizer.tokenize and map via token_to_id (UNK fallback).\n",
        "    - Split ids at mid-point; src = first half; ref = second half.\n",
        "    - Generate using ONLY src half as encoder input.\n",
        "    - Compare pred vs ref (BLEU/chrF).\n",
        "    \"\"\"\n",
        "    import torch\n",
        "    from sacrebleu.metrics import BLEU, CHRF\n",
        "\n",
        "    prev_training = model.training\n",
        "    if set_eval:\n",
        "        model.eval()\n",
        "\n",
        "    bleu = BLEU()\n",
        "    chrf = CHRF()\n",
        "    preds, refs = [], []\n",
        "\n",
        "    pad_id   = getattr(dataset, \"pad_idx\", tokenizer.token_to_id.get(\"<PAD>\"))\n",
        "    bos_id   = getattr(dataset, \"start_idx\", tokenizer.token_to_id.get(\"<START>\"))\n",
        "    eos_id   = getattr(dataset, \"end_idx\", tokenizer.token_to_id.get(\"<END>\"))\n",
        "    unk_id   = getattr(dataset, \"unk_idx\", tokenizer.token_to_id.get(\"<UNK>\"))\n",
        "\n",
        "    def ids_to_text(ids):\n",
        "        specials = {pad_id, bos_id, eos_id, unk_id}\n",
        "        toks = [tokenizer.id_to_token.get(i, \"<UNK>\") for i in ids if i not in specials]\n",
        "        return \"\".join(toks).replace(\"</w> \", \" \").replace(\"</w>\", \" \").strip()\n",
        "\n",
        "    n = min(sample_size, len(sentences))\n",
        "    for i in range(n):\n",
        "        s = sentences[i]\n",
        "\n",
        "        # tokenize → ids (same style as Dataset)\n",
        "        tok_list = tokenizer.tokenize(s)\n",
        "        ids = [tokenizer.token_to_id.get(t, unk_id) for t in tok_list]\n",
        "        if len(ids) < 4:\n",
        "            continue\n",
        "\n",
        "        split = len(ids) // 2\n",
        "        src_ids = ids[:split]\n",
        "        tgt_ids = ids[split:]\n",
        "\n",
        "        src_text = ids_to_text(src_ids)\n",
        "        ref_text = ids_to_text(tgt_ids)\n",
        "\n",
        "        pred_text = generate_text(\n",
        "            model, tokenizer, src_text, device,\n",
        "            max_len=max_gen_len, temperature=1.0, greedy=use_greedy\n",
        "        ).strip()\n",
        "\n",
        "        preds.append(pred_text)\n",
        "        refs.append(ref_text)\n",
        "\n",
        "    if set_eval and prev_training:\n",
        "        model.train()\n",
        "\n",
        "    if not preds:\n",
        "        return {\"bleu_score\": 0.0, \"chrf_score\": 0.0, \"n_eval\": 0}\n",
        "\n",
        "    bleu_score = float(bleu.corpus_score(preds, [refs]).score)\n",
        "    chrf_score = float(chrf.corpus_score(preds, [refs]).score)\n",
        "    return {\"bleu_score\": bleu_score, \"chrf_score\": chrf_score, \"n_eval\": len(preds)}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYpXh637sHTF"
      },
      "source": [
        "# Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UM5cyPwQsG2J"
      },
      "outputs": [],
      "source": [
        "def generate_text(\n",
        "    model, tokenizer, input_text, device,\n",
        "    max_len=50, temperature=1.0, top_k=None, top_p=None,\n",
        "    eos_token=\"<eos>\", greedy=False\n",
        "):\n",
        "    \"\"\"\n",
        "    Autoregressive decoding for your Transformer that does NOT rely on tokenizer.encode/decode\n",
        "    and does NOT call model.encoder()/model.decoder() (which don't exist).\n",
        "\n",
        "    - Uses tokenizer.tokenize + token_to_id / id_to_token.\n",
        "    - Uses <START>/<END>/<PAD> that were injected into tokenizer by the Dataset.\n",
        "    - Builds masks using model.make_src_mask / model.make_tgt_mask.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    model_was_training = model.training\n",
        "    model.eval()\n",
        "\n",
        "    # ---- Special IDs from tokenizer (injected by UrduChatbotDataset) ----\n",
        "    pad_id   = tokenizer.token_to_id.get(\"<PAD>\")\n",
        "    bos_id   = tokenizer.token_to_id.get(\"<START>\")\n",
        "    eos_id   = tokenizer.token_to_id.get(\"<END>\")\n",
        "    unk_id   = tokenizer.token_to_id.get(\"<UNK>\")\n",
        "\n",
        "    # ---- Encode source exactly like in Dataset.__getitem__ ----\n",
        "    src_tokens = tokenizer.tokenize(input_text)\n",
        "    src_ids = [tokenizer.token_to_id.get(t, unk_id) for t in src_tokens]\n",
        "    if len(src_ids) == 0:\n",
        "        return \"\"\n",
        "\n",
        "    # tensors\n",
        "    src = torch.tensor([src_ids], dtype=torch.long, device=device)\n",
        "    src_mask = model.make_src_mask(src)  # (B,1,1,src_len)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # ----- Encoder forward (explicit, since model has no .encoder()) -----\n",
        "        x = model.encoder_embedding(src) * math.sqrt(model.d_model)\n",
        "        x = model.encoder_pos_encoding(x)\n",
        "        for layer in model.encoder_layers:\n",
        "            x = layer(x, src_mask)\n",
        "        memory = x  # (B, S, d_model)\n",
        "\n",
        "        # ----- Decoder loop -----\n",
        "        ys = torch.tensor([[bos_id]], dtype=torch.long, device=device)  # (1,1)\n",
        "        out_ids = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            tgt_mask = model.make_tgt_mask(ys)  # (B,1,L,L)\n",
        "\n",
        "            x = model.decoder_embedding(ys) * math.sqrt(model.d_model)\n",
        "            x = model.decoder_pos_encoding(x)\n",
        "            for layer in model.decoder_layers:\n",
        "                x = layer(x, memory, src_mask, tgt_mask)\n",
        "\n",
        "            logits = model.output_projection(x[:, -1])  # (1,V)\n",
        "\n",
        "            if greedy:\n",
        "                next_id = int(torch.argmax(logits, dim=-1).item())\n",
        "            else:\n",
        "                probs = F.softmax(logits / max(1e-6, temperature), dim=-1)\n",
        "                # (optional) top-k / top-p could be added here if you want\n",
        "                next_id = int(torch.multinomial(probs, num_samples=1).item())\n",
        "\n",
        "            out_ids.append(next_id)\n",
        "\n",
        "            # append to sequence\n",
        "            ys = torch.cat([ys, torch.tensor([[next_id]], device=device)], dim=1)\n",
        "\n",
        "            if eos_id is not None and next_id == eos_id:\n",
        "                break\n",
        "\n",
        "    # ---- Detokenize: ids -> tokens -> text (strip specials, remove </w>) ----\n",
        "    specials = {pad_id, bos_id, eos_id, unk_id}\n",
        "    tokens = []\n",
        "    for tid in out_ids:\n",
        "        if tid in specials:\n",
        "            continue\n",
        "        tokens.append(tokenizer.id_to_token.get(tid, \"<UNK>\"))\n",
        "\n",
        "    text = \"\".join(tokens).replace(\"</w> \", \" \").replace(\"</w>\", \" \").strip()\n",
        "\n",
        "    if model_was_training:\n",
        "        model.train()\n",
        "\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wja3CA1rsQyL"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79RQ8D6VsTdM",
        "outputId": "8d5ff64f-3362-4485-df31-7c7045238f9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Loaded 20044 sentences\n",
            "Training BPE tokenizer...\n",
            "Initial vocab: 68 characters\n",
            "Final vocab: 7481 tokens\n",
            "Preparing dataset...\n",
            "Creating model...\n",
            "Parameters: 8,394,817\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "config = {\n",
        "    'vocab_size': 5000,\n",
        "    'd_model': 256,\n",
        "    'num_heads': 2,\n",
        "    'd_ff': 512,\n",
        "    'num_encoder_layers': 2,\n",
        "    'num_decoder_layers': 2,\n",
        "    'max_len': 50,\n",
        "    'dropout': 0.1,\n",
        "    'batch_size': 32,\n",
        "    'num_epochs': 100,\n",
        "    'learning_rate': 1e-4,\n",
        "}\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "data_path = os.path.join(base_dir, \"cleaned_urdu_text.txt\")\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    sentences = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "print(f\"Loaded {len(sentences)} sentences\")\n",
        "\n",
        "# Train tokenizer\n",
        "print(\"Training BPE tokenizer...\")\n",
        "tokenizer = BPETokenizer(vocab_size=config['vocab_size'])\n",
        "tokenizer.train(sentences[:5000], verbose=True)  # Subset for speed\n",
        "\n",
        "# Prepare dataset\n",
        "print(\"Preparing dataset...\")\n",
        "random.shuffle(sentences)\n",
        "train_size = int(0.8 * len(sentences))\n",
        "val_size = int(0.1 * len(sentences))\n",
        "\n",
        "train_sentences = sentences[:train_size]\n",
        "val_sentences = sentences[train_size:train_size + val_size]\n",
        "\n",
        "train_dataset = UrduChatbotDataset(train_sentences, tokenizer, config['max_len'])\n",
        "val_dataset = UrduChatbotDataset(val_sentences, tokenizer, config['max_len'])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "vocab_size = len(tokenizer.token_to_id) + 4\n",
        "\n",
        "# Create model\n",
        "print(\"Creating model...\")\n",
        "model = Transformer(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=config['d_model'],\n",
        "    num_heads=config['num_heads'],\n",
        "    d_ff=config['d_ff'],\n",
        "    num_encoder_layers=config['num_encoder_layers'],\n",
        "    num_decoder_layers=config['num_decoder_layers'],\n",
        "    max_len=config['max_len'],\n",
        "    dropout=config['dropout'],\n",
        "    pad_idx=train_dataset.pad_idx\n",
        ").to(device)\n",
        "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Training loop\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.pad_idx)\n",
        "optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], betas=(0.9, 0.98), eps=1e-9)\n",
        "#Used these values to make training faster and less smooth, encouraging quicker adaptation to new gradients.\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_bleu=-1.0\n",
        "\n",
        "for epoch in range(config['num_epochs']):\n",
        "    print(f\"Epoch {epoch + 1}/{config['num_epochs']}\")\n",
        "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss = validate(model, val_loader, criterion, device)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    ppl=math.exp(val_loss)\n",
        "    metrics = eval_bleu_chrf(\n",
        "    model, tokenizer, val_dataset, val_sentences,\n",
        "    max_gen_len=config['max_len'], device=device,\n",
        "    sample_size=2000\n",
        "    )\n",
        "    print(\n",
        "        f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "        f\"PPL : {ppl:.4f} | BLEU: {metrics['bleu_score']:.4f} | chrf: {metrics['chrf_score']:.4f} \"\n",
        "        f\"| n={metrics['n_eval']}\"\n",
        "    )\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        model_save_path = os.path.join(base_dir, \"best_model_notebook.pt\")\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "        print(\"★ Best model saved!\")\n",
        "\n",
        "    if metrics['bleu_score'] > best_bleu:\n",
        "        best_bleu = metrics['bleu_score']\n",
        "        model_save_path = os.path.join(base_dir, \"best_BLEU_model.pt\")\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "        print(\"★ New best BLEU — saved best model!\")\n",
        "    # if val_loss < 0.1:\n",
        "    #   break\n",
        "    # if val_loss - train_loss > 0.9:\n",
        "    #   break\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bf94HbFsV19"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nPg6TJmsXx-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4692a06f-c273-4517-81c6-203fb56635ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing generation:\n",
            "================================================================================\n",
            "Input: کبھی کبھار ہی\n",
            "Output: ہوں۔\n",
            "Input: آج موسم\n",
            "Output: ت نہیں\n"
          ]
        }
      ],
      "source": [
        "# Load best model\n",
        "best_model=os.path.join(base_dir, \"best_BLEU_model.pt\")\n",
        "model.load_state_dict(torch.load(best_model, map_location=device))\n",
        "\n",
        "# Test generation\n",
        "print(\"Testing generation:\")\n",
        "print(\"=\"*80)\n",
        "for text in [\"کبھی کبھار ہی\", \"آج موسم\"]:\n",
        "    out = generate_text(model, tokenizer, text,  device=device , max_len=30, greedy=True)\n",
        "    print(\"Input:\", text)\n",
        "    print(\"Output:\", out)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}