{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤– Urdu Conversational Chatbot with Transformer\\n\n",
    "\\n\n",
    "**Complete implementation from scratch using PyTorch**\\n\n",
    "\\n\n",
    "This notebook contains all components in one place:\\n\n",
    "- Data cleaning & normalization\\n\n",
    "- BPE tokenization\\n\n",
    "- Dataset preparation\\n\n",
    "- Transformer architecture\\n\n",
    "- Training loop\\n\n",
    "- Text generation\\n\n",
    "\\n\n",
    "Run cells sequentially to train your Urdu chatbot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm.notebook import tqdm  # Use notebook version\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\\n\n",
    "## 1. Data Cleaning & Normalization\\n\n",
    "\\n\n",
    "Normalize Urdu text by removing diacritics and standardizing character forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_urdu_text(text):\n",
    "    \"\"\"Normalize Urdu text by removing diacritics and standardizing forms\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove Arabic diacritical marks\n",
    "    diacritics = [\n",
    "        '\\u0610', '\\u0611', '\\u0612', '\\u0613', '\\u0614', '\\u0615', '\\u0616',\n",
    "        '\\u0617', '\\u0618', '\\u0619', '\\u061A', '\\u064B', '\\u064C', '\\u064D',\n",
    "        '\\u064E', '\\u064F', '\\u0650', '\\u0651', '\\u0652', '\\u0653', '\\u0654',\n",
    "        '\\u0655', '\\u0656', '\\u0657', '\\u0658', '\\u0659', '\\u065A', '\\u065B',\n",
    "        '\\u065C', '\\u065D', '\\u065E', '\\u065F', '\\u0670', '\\u06D6', '\\u06D7',\n",
    "        '\\u06D8', '\\u06D9', '\\u06DA', '\\u06DB', '\\u06DC', '\\u06DF', '\\u06E0',\n",
    "        '\\u06E1', '\\u06E2', '\\u06E3', '\\u06E4', '\\u06E7', '\\u06E8', '\\u06EA',\n",
    "        '\\u06EB', '\\u06EC', '\\u06ED'\n",
    "    ]\n",
    "    \n",
    "    for diacritic in diacritics:\n",
    "        text = text.replace(diacritic, '')\n",
    "    \n",
    "    # Standardize Alef and Yeh forms\n",
    "    text = re.sub('[Ø£Ø¥Ø¢Ù±]', 'Ø§', text)\n",
    "    text = re.sub('[ÙŠÙ‰Ø¦]', 'ÛŒ', text)\n",
    "    \n",
    "    # Remove zero-width characters\n",
    "    text = text.replace('\\u200c', '').replace('\\u200d', '')\n",
    "    \n",
    "    # Normalize punctuation\n",
    "    text = text.replace('\\u2018', \"'\").replace('\\u2019', \"'\")\n",
    "    text = text.replace('\\u201C', '\"').replace('\\u201D', '\"')\n",
    "    text = text.replace('\\u2026', '...').replace('\\u2013', '-').replace('\\u2014', '-')\n",
    "    text = text.replace('`', \"'\")\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\\n\n",
    "## 2. BPE Tokenizer\\n\n",
    "\\n\n",
    "Byte Pair Encoding with RTL-aware markers (`</w>` on left side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    \"\"\"Byte Pair Encoding tokenizer with RTL support\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 5000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = []\n",
    "        self.merges = []\n",
    "        self.token_to_id = {}\n",
    "        self.id_to_token = {}\n",
    "    \n",
    "    def get_stats(self, words: Dict[str, int]) -> Counter:\n",
    "        pairs = Counter()\n",
    "        for word, freq in words.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "        return pairs\n",
    "    \n",
    "    def merge_pair(self, pair: Tuple[str, str], words: Dict[str, int]) -> Dict[str, int]:\n",
    "        new_words = {}\n",
    "        bigram = ' '.join(pair)\n",
    "        replacement = ''.join(pair)\n",
    "        for word in words:\n",
    "            new_word = word.replace(bigram, replacement)\n",
    "            new_words[new_word] = words[word]\n",
    "        return new_words\n",
    "    \n",
    "    def train(self, corpus: List[str], verbose: bool = True):\n",
    "        \"\"\"Train BPE on corpus\"\"\"\n",
    "        word_freqs = Counter()\n",
    "        for text in corpus:\n",
    "            word_freqs.update(text.split())\n",
    "        \n",
    "        # RTL-aware: </w> at beginning\n",
    "        words = {}\n",
    "        for word, freq in word_freqs.items():\n",
    "            words['</w> ' + ' '.join(list(word))] = freq\n",
    "        \n",
    "        vocab = set()\n",
    "        for word in words.keys():\n",
    "            vocab.update(word.split())\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Initial vocab: {len(vocab)} characters\")\n",
    "        \n",
    "        num_merges = self.vocab_size - len(vocab)\n",
    "        for i in range(num_merges):\n",
    "            pairs = self.get_stats(words)\n",
    "            if not pairs:\n",
    "                break\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            words = self.merge_pair(best_pair, words)\n",
    "            self.merges.append(best_pair)\n",
    "        \n",
    "        final_vocab = set()\n",
    "        for word in words.keys():\n",
    "            final_vocab.update(word.split())\n",
    "        \n",
    "        self.vocab = sorted(list(final_vocab))\n",
    "        self.token_to_id = {token: idx for idx, token in enumerate(self.vocab)}\n",
    "        self.id_to_token = {idx: token for token, idx in self.token_to_id.items()}\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Final vocab: {len(self.vocab)} tokens\")\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize text using BPE\"\"\"\n",
    "        words = text.split()\n",
    "        tokens = []\n",
    "        for word in words:\n",
    "            word_tokens = ['</w>'] + list(word)\n",
    "            for pair in self.merges:\n",
    "                i = 0\n",
    "                while i < len(word_tokens) - 1:\n",
    "                    if (word_tokens[i], word_tokens[i + 1]) == pair:\n",
    "                        word_tokens = word_tokens[:i] + [''.join(pair)] + word_tokens[i + 2:]\n",
    "                    else:\n",
    "                        i += 1\n",
    "            tokens.extend(word_tokens)\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\\n\n",
    "## 3. Dataset Preparation\\n\n",
    "\\n\n",
    "Create PyTorch Dataset with full sequence next-token prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrduChatbotDataset(Dataset):\n",
    "    \"\"\"Dataset for Urdu chatbot with full sequence next-token prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, sentences, tokenizer, max_len=50):\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Special tokens\n",
    "        self.pad_token = '<PAD>'\n",
    "        self.start_token = '<START>'\n",
    "        self.end_token = '<END>'\n",
    "        self.unk_token = '<UNK>'\n",
    "        \n",
    "        for token in [self.pad_token, self.start_token, self.end_token, self.unk_token]:\n",
    "            if token not in self.tokenizer.token_to_id:\n",
    "                idx = len(self.tokenizer.token_to_id)\n",
    "                self.tokenizer.token_to_id[token] = idx\n",
    "                self.tokenizer.id_to_token[idx] = token\n",
    "        \n",
    "        self.pad_idx = self.tokenizer.token_to_id[self.pad_token]\n",
    "        self.start_idx = self.tokenizer.token_to_id[self.start_token]\n",
    "        self.end_idx = self.tokenizer.token_to_id[self.end_token]\n",
    "        self.unk_idx = self.tokenizer.token_to_id[self.unk_token]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        tokens = self.tokenizer.tokenize(sentence)\n",
    "        token_ids = [self.tokenizer.token_to_id.get(t, self.unk_idx) for t in tokens]\n",
    "        token_ids = token_ids[:self.max_len]\n",
    "        \n",
    "        # Full sequence for both encoder and decoder\n",
    "        encoder_ids = token_ids + [self.pad_idx] * (self.max_len - len(token_ids))\n",
    "        decoder_input_ids = [self.start_idx] + token_ids\n",
    "        decoder_input_ids = decoder_input_ids[:self.max_len] + [self.pad_idx] * (self.max_len - len(decoder_input_ids))\n",
    "        decoder_target_ids = token_ids + [self.end_idx]\n",
    "        decoder_target_ids = decoder_target_ids[:self.max_len] + [self.pad_idx] * (self.max_len - len(decoder_target_ids))\n",
    "        \n",
    "        return {\n",
    "            'encoder_input': torch.tensor(encoder_ids, dtype=torch.long),\n",
    "            'decoder_input': torch.tensor(decoder_input_ids, dtype=torch.long),\n",
    "            'decoder_target': torch.tensor(decoder_target_ids, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\\n\n",
    "## 4. Transformer Model Architecture\\n\n",
    "\\n\n",
    "Complete encoder-decoder implementation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding using sine and cosine\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention mechanism\"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        x = x.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 2)\n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        return output, attention_weights\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        Q = self.split_heads(self.W_q(query))\n",
    "        K = self.split_heads(self.W_k(key))\n",
    "        V = self.split_heads(self.W_v(value))\n",
    "        attn_output, _ = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(batch_size, -1, self.d_model)\n",
    "        output = self.W_o(attn_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise Feed-Forward Network\"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Single Transformer Encoder Layer\"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.self_attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout2(ff_output))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"Single Transformer Decoder Layer\"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.cross_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        self_attn_output = self.self_attention(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout1(self_attn_output))\n",
    "        cross_attn_output = self.cross_attention(x, encoder_output, encoder_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout2(cross_attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout3(ff_output))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"Complete Transformer Encoder-Decoder Model\"\"\"\n",
    "    def __init__(self, vocab_size, d_model=256, num_heads=8, d_ff=1024,\n",
    "                 num_encoder_layers=4, num_decoder_layers=4, max_len=512,\n",
    "                 dropout=0.1, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        self.encoder_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.encoder_pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
    "        self.decoder_pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "        \n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        self._init_parameters()\n",
    "    \n",
    "    def _init_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask\n",
    "    \n",
    "    def make_tgt_mask(self, tgt):\n",
    "        batch_size, tgt_len = tgt.size()\n",
    "        tgt_pad_mask = (tgt != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()\n",
    "        tgt_mask = tgt_pad_mask & tgt_sub_mask\n",
    "        return tgt_mask\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        tgt_mask = self.make_tgt_mask(tgt)\n",
    "        \n",
    "        # Encoder\n",
    "        x = self.encoder_embedding(src) * math.sqrt(self.d_model)\n",
    "        x = self.encoder_pos_encoding(x)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, src_mask)\n",
    "        encoder_output = x\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.decoder_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        x = self.decoder_pos_encoding(x)\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        \n",
    "        output = self.output_projection(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\\n\n",
    "## 5. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    pbar = tqdm(train_loader, desc=\"Training\")\n",
    "    \n",
    "    for batch in pbar:\n",
    "        encoder_input = batch['encoder_input'].to(device)\n",
    "        decoder_input = batch['decoder_input'].to(device)\n",
    "        decoder_target = batch['decoder_target'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(encoder_input, decoder_input)\n",
    "        output = output.reshape(-1, output.size(-1))\n",
    "        target = decoder_target.reshape(-1)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return epoch_loss / len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc=\"Validation\")\n",
    "        for batch in pbar:\n",
    "            encoder_input = batch['encoder_input'].to(device)\n",
    "            decoder_input = batch['decoder_input'].to(device)\n",
    "            decoder_target = batch['decoder_target'].to(device)\n",
    "            \n",
    "            output = model(encoder_input, decoder_input)\n",
    "            output = output.reshape(-1, output.size(-1))\n",
    "            target = decoder_target.reshape(-1)\n",
    "            loss = criterion(output, target)\n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return epoch_loss / len(val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\\n\n",
    "## 6. Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, dataset, input_text, max_length=30, temperature=0.8, device='cpu'):\n",
    "    \"\"\"Generate text continuation\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        tokens = tokenizer.tokenize(input_text)\n",
    "        token_ids = [tokenizer.token_to_id.get(t, dataset.unk_idx) for t in tokens]\n",
    "        encoder_ids = token_ids[:50] + [dataset.pad_idx] * (50 - len(token_ids))\n",
    "        encoder_input = torch.tensor([encoder_ids], dtype=torch.long).to(device)\n",
    "        decoder_input = torch.tensor([[dataset.start_idx]], dtype=torch.long).to(device)\n",
    "        \n",
    "        generated_tokens = []\n",
    "        for _ in range(max_length):\n",
    "            output = model(encoder_input, decoder_input)\n",
    "            next_token_logits = output[0, -1, :] / temperature\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            if next_token.item() == dataset.end_idx:\n",
    "                break\n",
    "            \n",
    "            generated_tokens.append(next_token.item())\n",
    "            decoder_input = torch.cat([decoder_input, next_token.unsqueeze(0)], dim=1)\n",
    "        \n",
    "        output_tokens = []\n",
    "        for idx in generated_tokens:\n",
    "            if idx not in [dataset.pad_idx, dataset.start_idx, dataset.unk_idx]:\n",
    "                token = tokenizer.id_to_token.get(idx, '<UNK>')\n",
    "                output_tokens.append(token)\n",
    "        \n",
    "        text = ''.join(output_tokens).replace('</w> ', ' ').replace('</w>', ' ')\n",
    "        return text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\\n\n",
    "## 7. Run Training\\n\n",
    "\\n\n",
    "Execute this cell to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = {\n",
    "    'vocab_size': 5000,\n",
    "    'd_model': 128,  # Smaller for notebook\n",
    "    'num_heads': 4,\n",
    "    'd_ff': 512,\n",
    "    'num_encoder_layers': 2,\n",
    "    'num_decoder_layers': 2,\n",
    "    'max_len': 50,\n",
    "    'dropout': 0.1,\n",
    "    'batch_size': 16,\n",
    "    'num_epochs': 5,\n",
    "    'learning_rate': 0.0001,\n",
    "}\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "with open('cleaned_urdu_text.txt', 'r', encoding='utf-8') as f:\n",
    "    sentences = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "print(f\"Loaded {len(sentences)} sentences\")\n",
    "\n",
    "# Train tokenizer\n",
    "print(\"\n",
    "Training BPE tokenizer...\")\n",
    "tokenizer = BPETokenizer(vocab_size=config['vocab_size'])\n",
    "tokenizer.train(sentences[:5000], verbose=True)  # Subset for speed\n",
    "\n",
    "# Prepare dataset\n",
    "print(\"Preparing dataset...\")\n",
    "random.shuffle(sentences)\n",
    "train_size = int(0.8 * len(sentences))\n",
    "val_size = int(0.1 * len(sentences))\n",
    "\n",
    "train_sentences = sentences[:train_size]\n",
    "val_sentences = sentences[train_size:train_size + val_size]\n",
    "\n",
    "train_dataset = UrduChatbotDataset(train_sentences, tokenizer, config['max_len'])\n",
    "val_dataset = UrduChatbotDataset(val_sentences, tokenizer, config['max_len'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "vocab_size = len(tokenizer.token_to_id) + 4\n",
    "\n",
    "# Create model\n",
    "print(\"Creating model...\")\n",
    "model = Transformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=config['d_model'],\n",
    "    num_heads=config['num_heads'],\n",
    "    d_ff=config['d_ff'],\n",
    "    num_encoder_layers=config['num_encoder_layers'],\n",
    "    num_decoder_layers=config['num_decoder_layers'],\n",
    "    max_len=config['max_len'],\n",
    "    dropout=config['dropout'],\n",
    "    pad_idx=train_dataset.pad_idx\n",
    ").to(device)\n",
    "\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Training loop\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.pad_idx)\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], betas=(0.9, 0.98))\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(config['num_epochs']):\n",
    "    print(f\"\n",
    "Epoch {epoch + 1}/{config['num_epochs']}\")\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model_notebook.pt')\n",
    "        print(\"â˜… Best model saved!\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\\n\n",
    "## 8. Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_model_notebook.pt', map_location=device))\n",
    "\n",
    "# Test generation\n",
    "test_inputs = [\"ÛŒÛ Ø§ÛŒÚ©\", \"Ù¾Ø§Ú©Ø³ØªØ§Ù† Ù…ÛŒÚº\", \"Ø§Ú†Ú¾Ø§\"]\n",
    "\n",
    "print(\"Testing generation:\")\n",
    "print(\"=\"*80)\n",
    "for input_text in test_inputs:\n",
    "    generated = generate_text(model, tokenizer, train_dataset, input_text, max_length=20, device=device)\n",
    "    print(f\"Input:  {input_text}\")\n",
    "    print(f\"Output: {generated}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
